{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification example inference with Ryzen AI\n",
    "\n",
    "This example demonstrates the 5 steps of classification model inference on the embedded Neural Processing Unit (NPU) in your AMD Ryzen AI enabled PC. The steps are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before starting, be sure you've installed the requirements listed in the requirements.txt file:\n",
    "!python -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get Model\n",
    "Here, we'll use the resnet50 model as an example. You may choose any classification models train with Imagenet from torchvision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from classification_utils import get_directories\n",
    "\n",
    "_, models_dir = get_directories()\n",
    "\n",
    "# load model from torchvision\n",
    "model = resnet50(weights=\"IMAGENET1K_V2\")\n",
    "\n",
    "# Save the model\n",
    "model.to(\"cpu\")\n",
    "torch.save(model, str(models_dir / \"resnet50.pt\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Export to ONNX\n",
    "\n",
    "The model inference with Ryzen AI is based on onnxruntime. The following code is used for exporting a PyTorch model to the ONNX (Open Neural Network Exchange) format. The ONNX file is needed to use the VitisAI Quantizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep for ONNX export\n",
    "dummy_inputs = torch.randn(1, 3, 224, 224)\n",
    "input_names = ['input']\n",
    "output_names = ['output']\n",
    "dynamic_axes = {'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    "tmp_model_path = str(models_dir / \"resnet50.onnx\")\n",
    "\n",
    "# Call export function\n",
    "torch.onnx.export(\n",
    "        model,\n",
    "        dummy_inputs,\n",
    "        tmp_model_path,\n",
    "        export_params=True,\n",
    "        opset_version=13,  # Recommended opset\n",
    "        input_names=input_names,\n",
    "        output_names=output_names,\n",
    "        dynamic_axes=dynamic_axes,\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Quantize Model\n",
    "\n",
    "Using the static quantization method provided by the Vitis AI Quantizer and providing the newly exported ONNX model, we'll quantize the model to INT8. For more information on this quantization method, see [Vitis AI ONNX Quantization](https://ryzenai.docs.amd.com/en/latest/vai_quant/vai_q_onnx.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import onnx\n",
    "import onnxruntime\n",
    "from onnxruntime.quantization import CalibrationDataReader, QuantType, QuantFormat, CalibrationMethod, quantize_static\n",
    "import vai_q_onnx\n",
    "\n",
    "data_dir = \"../../Ryzen-AI-Model-Zoo/dataset/imagenet\"\n",
    "\n",
    "# `input_model_path` is the path to the original, unquantized ONNX model.\n",
    "input_model_path = \"models/resnet50.onnx\"\n",
    "\n",
    "# `output_model_path` is the path where the quantized model will be saved.\n",
    "output_model_path = \"models/resnet50_quantized.onnx\"\n",
    "\n",
    "preprocess = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(256),\n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "calib_dir = os.path.join(data_dir, 'calib')\n",
    "calib_dataset = torchvision.datasets.ImageFolder(root=calib_dir, transform=preprocess)\n",
    "\n",
    "class ClassificationCalibrationDataReader(CalibrationDataReader):\n",
    "    def __init__(self, calib_dir: str, batch_size: int = 1):\n",
    "        super().__init__()\n",
    "        self.iterator = iter(DataLoader(calib_dir, batch_size))\n",
    "\n",
    "    def get_next(self) -> dict:\n",
    "        try:\n",
    "            images, labels = next(self.iterator)\n",
    "            return {\"input\": images.numpy()}\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "\n",
    "def classification_calibration_reader(calib_dir, batch_size=1):\n",
    "    return ClassificationCalibrationDataReader(calib_dir, batch_size=batch_size)\n",
    "\n",
    "dr = classification_calibration_reader(calib_dataset)\n",
    "\n",
    "vai_q_onnx.quantize_static(\n",
    "    input_model_path,\n",
    "    output_model_path,\n",
    "    dr,\n",
    "    quant_format=vai_q_onnx.QuantFormat.QDQ,\n",
    "    calibrate_method=vai_q_onnx.PowerOfTwoMethod.MinMSE,\n",
    "    activation_type=vai_q_onnx.QuantType.QUInt8,\n",
    "    weight_type=vai_q_onnx.QuantType.QInt8,\n",
    "    enable_ipu_cnn=True, \n",
    "    extra_options={'ActivationSymmetric': True} \n",
    ")\n",
    "print('Calibrated and quantized model saved at:', output_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model inference on CPU / iGPU / NPU with single image\n",
    "\n",
    "Now we have successfully quantized the model, and we will use the onnxruntime to do the inference on CPU, iGPU and NPU.\n",
    "A single image is applied to compare the execution time on different processors. The image pre and post processing function is defined as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import json\n",
    "\n",
    "# display images in notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms \n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def load_labels(path):\n",
    "    with open(path) as f:\n",
    "        data = json.load(f)\n",
    "    return np.asarray(data)\n",
    "\n",
    "def preprocess(input):\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "  \n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        normalize,\n",
    "    ])\n",
    "    img_tensor = transform(input).unsqueeze(0)\n",
    "    return img_tensor.numpy()\n",
    "    \n",
    "def softmax(x):\n",
    "    x = x.reshape(-1)\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def postprocess(result):\n",
    "    return softmax(np.array(result)).tolist()\n",
    "labels = load_labels('data/imagenet-simple-labels.json')\n",
    "image = Image.open('data/dog.jpg')\n",
    "\n",
    "print(\"Image size: \", image.size)\n",
    "plt.axis('off')\n",
    "display_image = plt.imshow(image)\n",
    "input_data = preprocess(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CPU Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Specify the path to the quantized ONNX Model\n",
    "onnx_model_path = \"models/resnet50_quantized.onnx\"\n",
    "\n",
    "cpu_options = onnxruntime.SessionOptions()\n",
    "\n",
    "# Create Inference Session to run the quantized model on the CPU\n",
    "cpu_session = onnxruntime.InferenceSession(\n",
    "    onnx_model_path,\n",
    "    providers = ['CPUExecutionProvider'],\n",
    "    sess_options=cpu_options,\n",
    ")\n",
    "start = time.time()\n",
    "cpu_outputs = cpu_session.run(None, {'input': input_data})\n",
    "end = time.time()\n",
    "\n",
    "cpu_results = postprocess(cpu_outputs)\n",
    "inference_time = np.round((end - start) * 1000, 2)\n",
    "idx = np.argmax(cpu_results)\n",
    "\n",
    "print('----------------------------------------')\n",
    "print('Final top prediction is: ' + labels[idx])\n",
    "print('----------------------------------------')\n",
    "\n",
    "print('----------------------------------------')\n",
    "print('Inference time: ' + str(inference_time) + \" ms\")\n",
    "print('----------------------------------------')\n",
    "\n",
    "sort_idx = np.flip(np.squeeze(np.argsort(cpu_results)))\n",
    "print('------------ Top 5 labels are: ----------------------------')\n",
    "print(labels[sort_idx[:5]])\n",
    "print('-----------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iGPU Inference\n",
    "\n",
    "We will leverage the onnxruntime DirectML ep to inference the model on AMD radeon 780m iGPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DML options\n",
    "dml_options = onnxruntime.SessionOptions()\n",
    "\n",
    "# Create Inference Session to run the quantized model on the iGPU\n",
    "dml_session = onnxruntime.InferenceSession(\n",
    "    onnx_model_path,\n",
    "    providers = ['DmlExecutionProvider'],\n",
    "    provider_options = [{\"device_id\": \"0\"}]\n",
    ")\n",
    "start = time.time()\n",
    "dml_outputs = dml_session.run(None, {'input': input_data})\n",
    "end = time.time()\n",
    "\n",
    "dml_results = postprocess(dml_outputs)\n",
    "inference_time = np.round((end - start) * 1000, 2)\n",
    "idx = np.argmax(dml_results)\n",
    "\n",
    "print('----------------------------------------')\n",
    "print('Final top prediction is: ' + labels[idx])\n",
    "print('----------------------------------------')\n",
    "\n",
    "print('----------------------------------------')\n",
    "print('Inference time: ' + str(inference_time) + \" ms\")\n",
    "print('----------------------------------------')\n",
    "\n",
    "sort_idx = np.flip(np.squeeze(np.argsort(dml_results)))\n",
    "print('------------ Top 5 labels are: ----------------------------')\n",
    "print(labels[sort_idx[:5]])\n",
    "print('-----------------------------------------------------------') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NPU Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
    "source": [
      "# Compile and run\n",
      "\n",
      "# Point to the config file path used for the VitisAI Execution Provider\n",
      "config_file_path = \"./vaip_config.json\"\n",
      "provider_options = [{\n",
      "              'config_file': config_file_path,\n",
      "              'ai_analyzer_visualization': True,\n",
      "              'ai_analyzer_profiling': True,\n",
      "          }]\n",
      "\n",
      "npu_session = onnxruntime.InferenceSession(\n",
      "    onnx_model_path,\n",
      "    providers = ['VitisAIExecutionProvider'],\n",
      "    provider_options = provider_options\n",
      ")\n",
      "\n",
      "start = time.time()\n",
      "npu_outputs = npu_session.run(None, {'input': input_data})\n",
      "end = time.time()\n",
      "\n",
      "npu_results = postprocess(npu_outputs)\n",
      "inference_time = np.round((end - start) * 1000, 2)\n",
      "idx = np.argmax(npu_results)\n",
      "\n",
      "print('----------------------------------------')\n",
      "print('Final top prediction is: ' + labels[idx])\n",
      "print('----------------------------------------')\n",
      "\n",
      "print('----------------------------------------')\n",
      "print('Inference time: ' + str(inference_time) + \" ms\")\n",
      "print('----------------------------------------')\n",
      "\n",
      "sort_idx = np.flip(np.squeeze(np.argsort(npu_results)))\n",
      "print('------------ Top 5 labels are: ----------------------------')\n",
      "print(labels[sort_idx[:5]])\n",
      "print('-----------------------------------------------------------')"
    ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model Analysis on NPU\n",
    "\n",
    "After NPU inference, there are several '.json' files generated by the Ryzen AI tracing tool, which could be open by the AI Analyzer for further optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aianalyzer ./ -p 8001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ryzen-ai-1.2.0",
   "language": "python",
   "name": "ryzen-ai-1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
